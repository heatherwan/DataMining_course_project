{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different perspective\n",
    "1. PCA\n",
    "2. manual\n",
    "   - with and without trustLevel\n",
    "3. automatic generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for test\n",
    "prepare different input dataset and test at 10-fold stratified cross validation set\n",
    "1. train_data (raw data)\n",
    "2. X_train_norm_enc data (normalized and encode)\n",
    "3. X_train_norm (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from classifiers import *\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 1879 entries and 10 features\n",
      "Test set has 498121 entries and 9 features\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv', sep = '|')\n",
    "test_data = pd.read_csv('data/test.csv', sep = '|')\n",
    "print(f'Train set has {train_data.shape[0]} entries and {train_data.shape[1]} features')\n",
    "print(f'Test set has {test_data.shape[0]} entries and {test_data.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "## normalize w/ encode \n",
    "y = train_data['fraud']\n",
    "X = train_data.drop(columns=['fraud']).astype(float)\n",
    "X_all = X.append(test_data, sort=False)\n",
    "X_all= pd.get_dummies(X_all, columns=['trustLevel'], prefix='trustLevel')\n",
    "X_norm_encode = pd.DataFrame(MinMaxScaler().fit_transform(X_all), columns=X_all.columns, index=X_all.index)\n",
    "print(X_norm_encode.shape)\n",
    "X_train_norm_enc = X_norm_encode.iloc[:1879,:]\n",
    "X_test_norm_enc = X_norm_encode.iloc[1879:,:]\n",
    "# print(X_train_norm_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 9)\n"
     ]
    }
   ],
   "source": [
    "## normalized w/o encode \n",
    "y = train_data['fraud']\n",
    "X = train_data.drop(columns=['fraud']).astype(float)\n",
    "X_all = X.append(test_data, sort=False)\n",
    "X_norm = pd.DataFrame(MinMaxScaler().fit_transform(X_all), columns=X_all.columns, index=X_all.index)\n",
    "print(X_norm.shape)\n",
    "X_train_norm = X_norm.iloc[:1879,:]\n",
    "X_test_norm = X_norm.iloc[1879:,:]\n",
    "# print(X_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_for_test(X,y):\n",
    "    \"\"\"\n",
    "    test code with 10-fold stratified cross validation\n",
    "    parameters\n",
    "    X: trainset features after generation\n",
    "    y: trainset y label\n",
    "    \"\"\"\n",
    "    evaluate_classification(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-55 -55 -55 -55 -50 -50 -50 -50 -50 -50]\n",
      "Linear SVM: test core = -520 \n",
      "[ -55  -55  -55  -80  -50  -50  -40  -75 -100  -50]\n",
      "RBF SVM: test core = -610 \n",
      "[ -55  -35  -60 -110  -40  -40  -55  -70  -90  -75]\n",
      "Logistic Regression: test core = -630 \n",
      "[-55 -45 -55 -35 -50 -50 -40 -75 -50 -50]\n",
      "Neural Net: test core = -505 \n",
      "[-55 -55 -55 -45 -50 -50 -40 -50 -50 -50]\n",
      "Random Forest: test core = -520 \n",
      "[ 35 -65 -15  25  -5 -35  10   5 -30  20]\n",
      "AdaBoost: test core = -55 \n",
      "[  0  10 -25   0 -20 -20 -15  15 -15  10]\n",
      "XGBoost: test core = -60 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(X_train_norm_enc, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
