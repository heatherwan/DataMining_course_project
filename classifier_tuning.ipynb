{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for classifier in cross classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "#     \"RBF SVM\": SVC(gamma=2, C=1),\n",
    "    \"Logistic Regression\": LogisticRegression( solver='lbfgs', max_iter = 500 ), \n",
    "    \"Neural Net\": MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=500),\n",
    "#     \"AdaBoost\" : AdaBoostClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "def profit_scorer(y, y_pred):\n",
    "#     print(confusion_matrix(y, y_pred))\n",
    "    profit_matrix = {(0,0): 0, (0,1): -5, (1,0): -25, (1,1): 5}\n",
    "    return sum(profit_matrix[(pred, actual)] for pred, actual in zip(y_pred, y))\n",
    "\n",
    "def evaluate_classification(X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    profit_scoring = make_scorer(profit_scorer, greater_is_better=True)\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "#         print(cross_validate(clf, X, y=y, cv=cv, scoring=profit_scoring)['test_score'])\n",
    "        result = sum(cross_validate(clf, X, y=y, cv=cv, scoring=profit_scoring)['test_score'])\n",
    "        print(f\"{name}: test core = {result} \")\n",
    "def code_for_test(X,y):\n",
    "    \"\"\"\n",
    "    test code with 10-fold stratified cross validation\n",
    "    parameters\n",
    "    X: trainset features after generation\n",
    "    y: trainset y label\n",
    "    \"\"\"\n",
    "    evaluate_classification(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y):\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=300)\n",
    "    models = {'logistic': model, 'RFE': RFE(model, 5), 'ridge': Ridge(alpha=1.0), 'Lasso': Lasso()}\n",
    "    for k, v in models.items():\n",
    "        print(k)\n",
    "        fit = v.fit(X, y)\n",
    "        if k == 'RFE':\n",
    "            print(\"Num Features: %s\" % (fit.n_features_))\n",
    "            print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "            for i, j in sorted(zip(fit.ranking_, X.columns), reverse=False):\n",
    "                print(i,j)\n",
    "\n",
    "        elif k == 'logistic':\n",
    "            for i, j in sorted(zip(map(lambda x: round(x, 4), fit.coef_.reshape(-1)), X.columns), reverse=True):\n",
    "                print(i,j)\n",
    "        else:\n",
    "            for i, j in sorted(zip(map(lambda x: round(x, 4), abs(fit.coef_)), X.columns), reverse=True):\n",
    "                print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for automatic feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_set = ['add', 'sub', 'mul', 'div', 'log', 'sqrt', 'abs', 'neg', 'max', 'min']  # \n",
    "def gp(X, y, gen, n_com, feature_set):\n",
    "    gp = SymbolicTransformer(generations=gen, population_size=2000,\n",
    "                             hall_of_fame=200, n_components=n_com,\n",
    "                             function_set=function_set,\n",
    "                             parsimony_coefficient='auto',\n",
    "                             max_samples=0.9, verbose=1,\n",
    "                             random_state=0, n_jobs=8,\n",
    "                             feature_names=X.columns)\n",
    "    x_gp = gp.fit_transform(X, y)\n",
    "    return x_gp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X_train, X_test, y_train, clf):\n",
    "    pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def cv_cost_semi(X, X_test_other, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    cost = {\"RBF SVM\": 0,\n",
    "    \"Logistic Regression\": 0, \n",
    "    \"Neural Net\": 0,\n",
    "    \"Random Forest\": 0,\n",
    "    \"AdaBoost\" : 0,\n",
    "    \"XGBoost\": 0}\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # get the split\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # generate coresponding test label\n",
    "        # model = LabelSpreading(gamma=10)\n",
    "        # model = LabelPropagation(alpha=None, gamma=10, kernel='rbf', max_iter=1000,n_jobs=None, n_neighbors=7, tol=0.001)\n",
    "        model = LabelSpreading(alpha=0.2, gamma=10, kernel='rbf', max_iter=30, n_jobs=None,n_neighbors=7, tol=0.001)\n",
    "        y_new_label = model.fit(X_train, y_train).predict(X_test_other)\n",
    "        X_all = X_train.append(X_test_other)\n",
    "        \n",
    "        y_all = y_train.append(pd.DataFrame(y_new_label))\n",
    "        # evaluation \n",
    "        for name, clf in classifiers.items():\n",
    "            pred = classify(X_all, X_test, y_all, clf)\n",
    "            cost[name] += profit_scorer(y_test, pred>0.5)\n",
    "    print(f'cost = {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for cost-sensitive xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_obj(y_hat, dtrain, alpha=5, beta=25): # alpha for FN beta for FP\n",
    "    y = dtrain.get_label()\n",
    "    pred = 1. / (1. + np.exp(-y_hat))\n",
    "    grad = pred * (beta + alpha*y - beta*y) - alpha*y  # alpha*(p-1)*y + beta*p*(1-y)\n",
    "    hess = pred * (1 - pred) * (beta + alpha*y - beta*y)\n",
    "    return grad, hess\n",
    "\n",
    "def err_rate(pred, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    loss_fn = y*np.log(pred)\n",
    "    loss_fp = (1.0 - y)*np.log(1.0 - pred)\n",
    "    return 'error', np.sum(-(5*loss_fn+25*loss_fp))/len(y)\n",
    "\n",
    "def cross_validation(X_train, X_test, y_train, y_test, depth, num_round):\n",
    "    # load data\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_val = xgb.DMatrix(X_test, label=y_test)\n",
    "    # build model\n",
    "    param = {'max_depth': depth, 'eta': 0.2, 'silent': 1, 'seed': 42, 'scale_pos_weight':1}\n",
    "#     watchlist = [(d_val, 'eval'), (d_train, 'train')]\n",
    "#     model_trn = xgb.train(param, d_train, num_round, watchlist, obj=logistic_obj, feval=err_rate)\n",
    "    model_trn = xgb.train(param, d_train, num_round, obj=logistic_obj, feval=err_rate)\n",
    "    # prediction\n",
    "    pred = model_trn.predict(d_val) \n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    return pred\n",
    "def cv_cost_xg(X,y, depth, rounds):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    cost=0\n",
    "    if type(X) == pd.DataFrame:\n",
    "        X=X.values\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pred = cross_validation(X_train, X_test, y_train, y_test, depth, rounds)\n",
    "#         print(f' confusion matrix: cost({profit_scorer(y_test, pred>0.5)})\\n {confusion_matrix(y_test, pred>0.5)}')\n",
    "        cost += profit_scorer(y_test, pred>0.5)\n",
    "\n",
    "    print(f'cost = {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for test\n",
    "prepare different input dataset and test at 10-fold stratified cross validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. X (raw data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 1879 entries and 10 features\n",
      "Test set has 498121 entries and 9 features\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv', sep = '|')\n",
    "test_data = pd.read_csv('data/test.csv', sep = '|')\n",
    "print(f'Train set has {train_data.shape[0]} entries and {train_data.shape[1]} features')\n",
    "print(f'Test set has {test_data.shape[0]} entries and {test_data.shape[1]} features')\n",
    "y = train_data['fraud']\n",
    "X = train_data.drop(columns=['fraud']).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode trustlevel 1,2,other\n",
    "X_encode = pd.get_dummies(X, columns=['trustLevel'], prefix='trust')\n",
    "X_encode = X_encode.assign(trust_other = (X_encode['trust_3.0']+X_encode['trust_4.0']+X_encode['trust_5.0']+X_encode['trust_6.0']))\n",
    "X_encode = X_encode.drop(columns=['trust_3.0', 'trust_4.0', 'trust_5.0','trust_6.0'])\n",
    "## test data\n",
    "X_encode_test = pd.get_dummies(test_data, columns=['trustLevel'], prefix='trust')\n",
    "X_encode_test = X_encode_test.assign(trust_other = (X_encode_test['trust_3']+X_encode_test['trust_4']+X_encode_test['trust_5']+X_encode_test['trust_6']))\n",
    "X_encode_test = X_encode_test.drop(columns=['trust_3', 'trust_4', 'trust_5','trust_6'])\n",
    "# # normalize\n",
    "# fit_minmax = MinMaxScaler()\n",
    "# X_encode_norm = pd.DataFrame(fit_minmax.fit_transform(X_encode), columns=X_encode.columns, index=X_encode.index)\n",
    "# X_encode_norm_test = pd.DataFrame(fit_minmax.transform(X_encode_test), columns=X_encode_test.columns, index=X_encode_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_manual = X_encode.assign(no_item = X_encode.totalScanTimeInSeconds* X_encode.scannedLineItemsPerSecond)\\\n",
    "                     .drop(columns=['grandTotal','quantityModifications'])\n",
    "X_manual = pd.DataFrame(fit_minmax.fit_transform(X_manual), columns=X_manual.columns, index=X_manual.index)\n",
    "\n",
    "X_manual_test = X_encode_test.assign(no_item = X_encode_test.totalScanTimeInSeconds* X_encode_test.scannedLineItemsPerSecond)\\\n",
    "                     .drop(columns=['grandTotal','quantityModifications'])\n",
    "X_manual_test = pd.DataFrame(fit_minmax.fit_transform(X_manual_test), columns=X_manual_test.columns, index=X_manual_test.index)\n",
    "\n",
    "pca = PCA(n_components = 'mle')\n",
    "X_manual_PCA = pca.fit_transform(X_manual)\n",
    "X_manual_test_PCA = pca.transform(X_manual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = 260 \n",
      "Neural Net: test core = 180 \n",
      "XGBoost: test core = 145 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(X_manual, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = 260 \n",
      "Neural Net: test core = 210 \n",
      "XGBoost: test core = 70 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(X_manual_PCA, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto generate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    12.64        0.0966508       20         0.513866         0.595703     37.45s\n",
      "   1    10.10         0.278288       21         0.534857         0.403993      2.48m\n",
      "   2     7.65         0.359981       33         0.555662         0.507783      3.23m\n",
      "   3     7.21         0.387842        6         0.584537         0.568708      3.04m\n",
      "   4    10.08         0.366341        5          0.66279         0.641416      2.98m\n",
      "   5     6.06         0.407309       12          0.71989         0.647714      3.18m\n",
      "   6     6.18         0.434667       14         0.788198         0.464811      2.68m\n",
      "   7     5.88         0.476818       12         0.795699          0.70004      2.54m\n",
      "   8     5.04          0.43692        9         0.760092          0.50608      2.41m\n",
      "   9     4.99          0.42931        9         0.743437         0.621731      2.30m\n",
      "  10     4.93         0.424169        9         0.746791         0.650724      2.18m\n",
      "  11     4.90         0.414151        9         0.750831         0.613517      2.04m\n",
      "  12     4.96         0.417972        9         0.750593         0.553037      1.93m\n",
      "  13     5.03         0.425841        9         0.755724         0.588141      1.80m\n",
      "  14     5.03         0.424346        9         0.747823         0.546136      1.79m\n",
      "  15     4.94         0.440826        9         0.746304         0.513161      1.57m\n",
      "  16     4.97         0.435064        9         0.754179         0.478954      1.49m\n",
      "  17     4.94         0.434945        9          0.74845         0.581546      1.38m\n",
      "  18     4.92         0.434291        9         0.754126         0.557125      1.28m\n",
      "  19     5.00         0.452539        9         0.749873         0.627969      1.18m\n",
      "  20     5.11         0.453259        9         0.760381         0.471131      1.05m\n",
      "  21     4.98         0.439161        9         0.741906         0.595467     56.57s\n",
      "  22     4.97          0.44233        9         0.748258         0.523697     48.82s\n",
      "  23     4.35         0.441857        9          0.74666         0.492938     45.86s\n",
      "  24     4.95         0.445741        9         0.757818         0.487657     33.84s\n",
      "  25     4.55         0.453246        9         0.745914         0.629102     28.01s\n",
      "  26     2.89         0.418961        9         0.749799         0.597324     22.79s\n",
      "  27     5.00         0.460321        9         0.754652         0.539915     13.89s\n",
      "  28     4.48         0.444808        9         0.744303         0.612641      7.28s\n",
      "  29     5.01         0.469758        9         0.757666         0.408448      0.00s\n"
     ]
    }
   ],
   "source": [
    "x_auto = gp(X_encode_norm, y, 30, 100, ['div', 'mul', 'abs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = -520 \n",
      "Neural Net: test core = -520 \n",
      "XGBoost: test core = -65 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(x_auto, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semi-supervised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_cost_semi(X_manual, X_manual_test.iloc[:,:30000], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define xgboost with cost sensitive\n",
    "1. better than original sgboost but worse than other algorithm \n",
    "2. best score 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 270\n"
     ]
    }
   ],
   "source": [
    "cv_cost_xg(X_manual, y, 3 ,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
