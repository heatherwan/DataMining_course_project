{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_predict\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from mlens.ensemble import SuperLearner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for classifier in cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='lbfgs', max_iter = 500, C=1, penalty='l2'), \n",
    "    \"Neural Net\": MLPClassifier(alpha=1, max_iter=1000),\n",
    "    \"XGBoost\": xgb.XGBClassifier(learning_rate =0.1, n_estimators=100, max_depth=3,\n",
    "                                 min_child_weight=2, gamma=0, subsample=0.7, colsample_bytree=0.8, seed=27,\n",
    "                                 reg_alpha=0.05, reg_lambda=1)\n",
    "}\n",
    "def profit_scorer(y, y_pred):\n",
    "#     print(confusion_matrix(y, y_pred))\n",
    "    profit_matrix = {(0,0): 0, (0,1): -5, (1,0): -25, (1,1): 5}\n",
    "    return sum(profit_matrix[(pred, actual)] for pred, actual in zip(y_pred, y))\n",
    "\n",
    "def evaluate_classification(X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    profit_scoring = make_scorer(profit_scorer, greater_is_better=True)\n",
    "    for name, clf in classifiers.items():\n",
    "        result = sum(cross_validate(clf, X, y=y, cv=cv, scoring=profit_scoring)['test_score'])\n",
    "        print(f\"{name}: test core = {result} \")\n",
    "\n",
    "def code_for_test(X,y):\n",
    "    \"\"\"\n",
    "    test code with 10-fold stratified cross validation\n",
    "    parameters\n",
    "    X: trainset features after generation\n",
    "    y: trainset y label\n",
    "    \"\"\"\n",
    "    evaluate_classification(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for automatic feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_set = ['add', 'sub', 'mul', 'div', 'log', 'sqrt', 'abs', 'neg', 'max', 'min']  # \n",
    "def gp(X, y, gen, n_com):\n",
    "    gp1 = SymbolicTransformer(generations=gen, population_size=1000,\n",
    "                             hall_of_fame=1000, n_components=n_com,\n",
    "                             function_set=function_set,\n",
    "                             parsimony_coefficient=0.0005,\n",
    "                             max_samples=0.9, verbose=1,\n",
    "                             random_state=42, n_jobs=3,\n",
    "                             feature_names=X.columns)\n",
    "    x_gp = gp1.fit_transform(X, y)\n",
    "    code_for_test(x_gp, y)\n",
    "    return gp1, x_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X_train, X_test, y_train, clf):\n",
    "    pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def cv_cost_semi(X, X_test_other, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    cost = {\n",
    "    \"Logistic Regression\": 0, \n",
    "    \"Neural Net\": 0,\n",
    "    \"XGBoost\": 0,\n",
    "    \"ensemble\": 0}\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # get the split\n",
    "        X_train, X_test = X[train_index,:], X[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # generate coresponding test label\n",
    "        # model = LabelSpreading(gamma=10)\n",
    "        model = LabelPropagation(alpha=None, gamma=10, kernel='rbf', max_iter=1000,n_jobs=None, n_neighbors=7, tol=0.001)\n",
    "#         model = LabelSpreading(alpha=0.2, gamma=10, kernel='rbf', max_iter=30, n_jobs=None,n_neighbors=7, tol=0.001)\n",
    "        y_new_label = model.fit(X_train, y_train).predict(X_test_other)\n",
    "        X_all = np.vstack((X_train,X_test_other))\n",
    "        \n",
    "        y_all = y_train.append(pd.DataFrame(y_new_label))\n",
    "        # evaluation \n",
    "        for name, clf in classifiers.items():\n",
    "            pred = classify(X_all, X_test, y_all, clf)\n",
    "            cost[name] += profit_scorer(y_test, pred>0.5)\n",
    "    print(f'cost = {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for cost-sensitive xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost sensitive objective function and derivative\n",
    "def logistic_obj(y_hat, dtrain, alpha=5, beta=25): # alpha for FN beta for FP\n",
    "    y = dtrain.get_label()\n",
    "    pred = 1. / (1. + np.exp(-y_hat))\n",
    "    grad = pred * (beta + alpha*y - beta*y) - alpha*y  # alpha*(p-1)*y + beta*p*(1-y)\n",
    "    hess = pred * (1 - pred) * (beta + alpha*y - beta*y)\n",
    "    return grad, hess\n",
    "\n",
    "# calculate the error\n",
    "def err_rate(pred, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    loss_fn = y*np.log(pred)\n",
    "    loss_fp = (1.0 - y)*np.log(1.0 - pred)\n",
    "    return 'error', np.sum(-(5*loss_fn+25*loss_fp))/len(y)\n",
    "\n",
    "# train on one cross_validation split\n",
    "def cross_validation(X_train, X_test, y_train, y_test, depth, num_round):\n",
    "    # load data\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_val = xgb.DMatrix(X_test, label=y_test)\n",
    "    # build model\n",
    "    param = {'max_depth': depth, 'eta': 0.2, 'silent': 1, 'seed': 42, 'scale_pos_weight':1}\n",
    "#     watchlist = [(d_val, 'eval'), (d_train, 'train')]\n",
    "#     model_trn = xgb.train(param, d_train, num_round, watchlist, obj=logistic_obj, feval=err_rate)\n",
    "    model_trn = xgb.train(param, d_train, num_round, obj=logistic_obj, feval=err_rate)\n",
    "    # prediction\n",
    "    pred = model_trn.predict(d_val) \n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    return pred\n",
    "\n",
    "# train on all cross-validation set\n",
    "def cv_cost_xg(X,y, depth, rounds):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    cost=0\n",
    "    if type(X) == pd.DataFrame:\n",
    "        X=X.values\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pred = cross_validation(X_train, X_test, y_train, y_test, depth, rounds)\n",
    "        cost += profit_scorer(y_test, pred>0.5)\n",
    "    print(f'cost = {cost}')\n",
    "\n",
    "#train on all train set   \n",
    "def train_xgb(X_train, y_train, X_test):\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_test = xgb.DMatrix(X_test)\n",
    "    param = {'max_depth': 3, 'eta': 0.2, 'silent': 1, 'seed': 42, 'scale_pos_weight':1}\n",
    "    model_trn = xgb.train(param, d_train, 100, obj=logistic_obj, feval=err_rate)\n",
    "    pred = model_trn.predict(d_train)\n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    print(f'cost on train = {profit_scorer(y, pred>0.5)}')\n",
    "    print(confusion_matrix(y, pred>0.5))\n",
    "    pred_test = model_trn.predict(d_test)\n",
    "    pred_test = 1. / (1. + np.exp(-pred_test))\n",
    "    return model_trn, pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for test\n",
    "prepare different input dataset and test at 10-fold stratified cross validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 1879 entries and 10 features\n",
      "Test set has 498121 entries and 9 features\n"
     ]
    }
   ],
   "source": [
    "#raw data\n",
    "train_data = pd.read_csv('data/train.csv', sep = '|')\n",
    "test_data = pd.read_csv('data/test.csv', sep = '|')\n",
    "print(f'Train set has {train_data.shape[0]} entries and {train_data.shape[1]} features')\n",
    "print(f'Test set has {test_data.shape[0]} entries and {test_data.shape[1]} features')\n",
    "y = train_data['fraud']\n",
    "X = train_data.drop(columns=['fraud']).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete correlate features\n",
    "X_manual = X.assign(no_item = X.totalScanTimeInSeconds* X.scannedLineItemsPerSecond)\\\n",
    "                     .drop(columns=['valuePerSecond', 'lineItemVoidsPerPosition','scannedLineItemsPerSecond'])\n",
    "X_test = test_data.assign(no_item = test_data.totalScanTimeInSeconds* test_data.scannedLineItemsPerSecond)\\\n",
    "                    .drop(columns=['valuePerSecond', 'lineItemVoidsPerPosition','scannedLineItemsPerSecond'])\n",
    "\n",
    "fit_minmax = MinMaxScaler()\n",
    "# normalize with encode\n",
    "X_manual_encode = pd.get_dummies(X_manual, columns=['trustLevel'], prefix='trustLevel')\n",
    "X_test_encode = pd.get_dummies(X_test, columns=['trustLevel'], prefix='trustLevel')\n",
    "X_train_manual_enc = pd.DataFrame(fit_minmax.fit_transform(X_manual_encode), columns=X_manual_encode.columns, index=X_manual_encode.index)\n",
    "X_test_manual_enc = pd.DataFrame(fit_minmax.transform(X_test_encode), columns=X_manual_encode.columns, index=X_test_encode.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = 205 \n",
      "Neural Net: test core = 200 \n",
      "XGBoost: test core = 215 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(X_train_manual_enc,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto generate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    11.86         0.094761        9         0.707305         0.651601      4.79m\n",
      "   1     9.58         0.276511       14         0.722464         0.744651      2.58m\n",
      "   2     8.42         0.469678       14         0.783092         0.807669      2.73m\n",
      "   3    11.46          0.63334       17         0.808567         0.733662      2.75m\n",
      "   4    15.33          0.69389       31         0.813072         0.626655      2.98m\n",
      "   5    17.37         0.712183       17         0.818479         0.644542      3.30m\n",
      "   6    18.83         0.732394       21         0.823698         0.546554      3.27m\n",
      "   7    19.90         0.747221       34         0.823915         0.659249      3.07m\n",
      "   8    19.84         0.742844       30         0.827044          0.59718      3.52m\n",
      "   9    21.02         0.740488       31         0.824565         0.717744      3.45m\n",
      "  10    22.25         0.746554       19         0.838084         0.493703      3.81m\n",
      "  11    22.36         0.738917       34         0.827467         0.671664      3.83m\n",
      "  12    24.18         0.741468       32         0.836429         0.576184      3.80m\n",
      "  13    26.13         0.749584       46         0.833621         0.591198      3.47m\n",
      "  14    26.94         0.744273       30         0.832481         0.435813      3.26m\n",
      "  15    28.73         0.744509       33         0.846065         0.722157      3.46m\n",
      "  16    31.51         0.732658       57         0.850064         0.789959      3.49m\n",
      "  17    38.60         0.755411       71         0.861799         0.766309      3.84m\n",
      "  18    44.67         0.749204       72         0.868726         0.689672      3.91m\n",
      "  19    46.56         0.756051       47         0.865465         0.696961      3.73m\n",
      "  20    44.01         0.748449       60         0.867603         0.659423      3.62m\n",
      "  21    43.03         0.753184       90         0.875551         0.709686      3.81m\n",
      "  22    45.70         0.742028       58         0.883662         0.808539      4.10m\n",
      "  23    46.77          0.75739       54         0.892129          0.70354      4.92m\n",
      "  24    52.40         0.755799       61         0.894628         0.743022      3.39m\n",
      "  25    56.09         0.781226       82          0.89397         0.806372      3.67m\n",
      "  26    58.02         0.777707       64         0.895447         0.736113      3.92m\n",
      "  27    59.95         0.784839      113         0.897958         0.844272      3.96m\n",
      "  28    58.44         0.782826       87         0.903513         0.793527      4.06m\n",
      "  29    58.49         0.786378       61         0.900483         0.727905      4.07m\n",
      "  30    59.44         0.782333       58         0.898721          0.82965      5.47m\n",
      "  31    58.98         0.792305       86          0.90048         0.642479      4.26m\n",
      "  32    57.46         0.790521       58         0.900004         0.815643      4.07m\n",
      "  33    59.55         0.787406       64         0.902434         0.596415      3.99m\n",
      "  34    59.75         0.786541      117         0.907439         0.910043      3.79m\n",
      "  35    59.58         0.786774       82         0.911093         0.785309      4.19m\n",
      "  36    59.86         0.784567       76         0.909101         0.850181      3.96m\n",
      "  37    63.88         0.791494      113         0.916658         0.850529      3.61m\n",
      "  38    61.06         0.794154       80         0.919694         0.779325      3.74m\n",
      "  39    63.91         0.797037       94         0.918988         0.867832      3.70m\n",
      "  40    63.58         0.800251       92         0.922618         0.824741      3.72m\n",
      "  41    66.22         0.797452       93         0.926399         0.841443      3.66m\n",
      "  42    66.06         0.805593      112         0.927722         0.848753      3.41m\n",
      "  43    67.33         0.810735       84         0.931747         0.812813      3.21m\n",
      "  44    66.19         0.811073       70         0.925198         0.856129      3.27m\n",
      "  45    66.31         0.811157      122         0.936718         0.821544      3.36m\n",
      "  46    67.65         0.811055       70         0.927351         0.835218      3.27m\n",
      "  47    66.82         0.804427      115         0.928749         0.868988      3.28m\n",
      "  48    66.38         0.807848      119         0.930937         0.888733      3.40m\n",
      "  49    64.76         0.797558       89         0.929507         0.867553      3.15m\n",
      "  50    65.67         0.804981      118         0.934219         0.861935      3.37m\n",
      "  51    66.18         0.808393      113         0.933255         0.888399      2.99m\n",
      "  52    65.93         0.814224       72         0.928948         0.667106      3.05m\n",
      "  53    64.23           0.8079       94         0.935105         0.886364      2.96m\n",
      "  54    64.88         0.813003       94         0.934817         0.889159      2.81m\n",
      "  55    64.61         0.810727      125         0.935516         0.772798      2.87m\n",
      "  56    65.03         0.811185       98         0.934245         0.916868      2.52m\n",
      "  57    63.72         0.800992       91           0.9347         0.783956      2.50m\n",
      "  58    62.91         0.799129       79          0.92901         0.790454      2.56m\n",
      "  59    63.30         0.805414      124         0.927962         0.854788      2.73m\n",
      "  60    61.32         0.793513      125         0.932075         0.889023      2.63m\n",
      "  61    60.48         0.796542      105          0.92892         0.872942      2.85m\n",
      "  62    59.97          0.79488      108         0.930733         0.853904      2.48m\n",
      "  63    57.61         0.784157       75         0.931114         0.835326      2.28m\n",
      "  64    55.29         0.777094       64         0.926278         0.846753      2.62m\n",
      "  65    55.41         0.791718      105         0.933556         0.882692      2.03m\n",
      "  66    55.43         0.790854       70         0.931059         0.866295      1.92m\n",
      "  67    54.18         0.780015      100         0.931609         0.794665      1.87m\n",
      "  68    54.90         0.781967       56         0.928658          0.83417      1.86m\n",
      "  69    52.67         0.769551       76         0.929898         0.795719      1.68m\n",
      "  70    51.17         0.777019       49         0.927594         0.810172      1.71m\n",
      "  71    51.39         0.774404       73         0.927614          0.92489      1.58m\n",
      "  72    52.02         0.779888       51         0.929635         0.709492      1.47m\n",
      "  73    50.97         0.762514       56         0.933098         0.712223      1.45m\n",
      "  74    51.83         0.777264       96         0.930084         0.899021      1.34m\n",
      "  75    51.30         0.775547       58         0.926421         0.835953      1.28m\n",
      "  76    49.95         0.773139       49         0.927092         0.777819      1.22m\n",
      "  77    49.62         0.770494       47         0.928207         0.718479      1.25m\n",
      "  78    50.06         0.769071       50         0.926018         0.888128      1.27m\n",
      "  79    50.67         0.770827       49          0.92608          0.84951      1.13m\n",
      "  80    50.58         0.766848       49         0.925787         0.796901      1.15m\n",
      "  81    49.37         0.762808       49         0.924861         0.800407      1.04m\n",
      "  82    49.66         0.775629       51         0.926941         0.845837     59.80s\n",
      "  83    48.99         0.754215       93         0.926806         0.847406     52.20s\n",
      "  84    49.80         0.773212      106         0.937091         0.867728     58.15s\n",
      "  85    49.84         0.766667       49          0.92544         0.842729     48.07s\n",
      "  86    48.76         0.758996       52          0.92545         0.860824     42.53s\n",
      "  87    50.27          0.77009       90         0.925791         0.861164     39.97s\n",
      "  88    51.60         0.772809      101          0.92995         0.856011     37.08s\n",
      "  89    50.44         0.768987      104         0.931084          0.83502     31.28s\n",
      "  90    50.32         0.769384       56         0.925415         0.831891     32.55s\n",
      "  91    49.46         0.762435       49         0.927643          0.79841     29.59s\n",
      "  92    47.83         0.758726       91         0.932408         0.881593     23.95s\n",
      "  93    49.96         0.761948       87         0.931054         0.931789     21.53s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94    50.18         0.770984      106          0.92963          0.95864     17.65s\n",
      "  95    49.72         0.759817       70          0.93057          0.86936     12.87s\n",
      "  96    50.59         0.769957       50         0.933293         0.795149      9.78s\n",
      "  97    49.91          0.76969       70         0.934764         0.753906      6.77s\n",
      "  98    50.27          0.76229       74         0.937564         0.865825      3.66s\n",
      "  99    49.87         0.767216       70         0.938455         0.707139      0.00s\n",
      "Logistic Regression: test core = 310 \n",
      "Neural Net: test core = 340 \n",
      "XGBoost: test core = 300 \n"
     ]
    }
   ],
   "source": [
    "gpresult, xgp = gp(X_train_manual_enc, y, 100, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble (stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = SuperLearner()\n",
    "ensemble.add(list(classifiers.values()))\n",
    "ensemble.add_meta(LogisticRegression())\n",
    "classifiers['ensemble'] = ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = 310 \n",
      "Neural Net: test core = 330 \n",
      "XGBoost: test core = 300 \n",
      "ensemble: test core = 325 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(xgp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define xgboost with cost sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 325\n"
     ]
    }
   ],
   "source": [
    "cv_cost_xg(xgp, y, 3 ,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semi-supervised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test12 = X_test_manual_enc[X_test_manual_enc['trustLevel_1.0']==1].append(X_test_manual_enc[X_test_manual_enc['trustLevel_2.0']==1])\n",
    "xgp_test12 = gpresult.transform(x_test12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = {'Logistic Regression': 275, 'Neural Net': 280, 'XGBoost': 230, 'ensemble': 240}\n"
     ]
    }
   ],
   "source": [
    "cv_cost_semi(xgp, xgp_test12, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(x,y,x_test, clf):\n",
    "    clf.fit(x,y)\n",
    "    y_pred = clf.predict(x)\n",
    "    print(f'cost on train = {profit_scorer(y, y_pred>0.5)}')\n",
    "    print(confusion_matrix(y, y_pred>0.5))\n",
    "    prediction = clf.predict(x_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgp_test = gpresult.transform(X_test_manual_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "cost on train = 375\n",
      "[[1772    3]\n",
      " [   7   97]]\n",
      "result on test: Counter({0: 475423, 1: 22698})\n",
      "Neural Net\n",
      "cost on train = 350\n",
      "[[1773    2]\n",
      " [  12   92]]\n",
      "result on test: Counter({0: 476562, 1: 21559})\n",
      "XGBoost\n",
      "cost on train = 485\n",
      "[[1774    1]\n",
      " [   1  103]]\n",
      "result on test: Counter({0: 473756, 1: 24365})\n",
      "ensemble\n",
      "cost on train = 425\n",
      "[[1774    1]\n",
      " [   7   97]]\n",
      "result on test: Counter({0.0: 475905, 1.0: 22216})\n",
      "XGBoost cost sensitive\n",
      "cost on train = 520\n",
      "[[1775    0]\n",
      " [   0  104]]\n",
      "result on test: Counter({False: 474345, True: 23776})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,5)\n",
    "fig.set_figheight(3)\n",
    "fig.set_figwidth(15)\n",
    "i = 0\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(name)\n",
    "    prediction = final_model(xgp, y, xgp_test, clf)\n",
    "    np.savetxt(f\"Result/fraud_{name}.csv\", (prediction>0.5), delimiter=\",\", fmt='%d')\n",
    "    ax[i].hist(prediction)\n",
    "    i+=1\n",
    "    print(f'result on test: {collections.Counter(prediction)}')\n",
    "\n",
    "print('XGBoost cost sensitive')\n",
    "model, cost_xgb_pred = train_xgb(xgp, y, xgp_test)\n",
    "np.savetxt(f\"Result/fraud_XGBoost cost sensitive.csv\", (cost_xgb_pred>0.5), delimiter=\",\", fmt='%d')\n",
    "ax[4].hist(cost_xgb_pred)\n",
    "print(f'result on test: {collections.Counter(cost_xgb_pred>0.5)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
