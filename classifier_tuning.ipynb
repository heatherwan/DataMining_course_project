{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for classifier in cross classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "#     \"RBF SVM\": SVC(gamma=2, C=1),\n",
    "    \"Logistic Regression\": LogisticRegression( solver='lbfgs', max_iter = 500 ), \n",
    "    \"Neural Net\": MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=500),\n",
    "#     \"AdaBoost\" : AdaBoostClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "def profit_scorer(y, y_pred):\n",
    "#     print(confusion_matrix(y, y_pred))\n",
    "    profit_matrix = {(0,0): 0, (0,1): -5, (1,0): -25, (1,1): 5}\n",
    "    return sum(profit_matrix[(pred, actual)] for pred, actual in zip(y_pred, y))\n",
    "\n",
    "def evaluate_classification(X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    profit_scoring = make_scorer(profit_scorer, greater_is_better=True)\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "#         print(cross_validate(clf, X, y=y, cv=cv, scoring=profit_scoring)['test_score'])\n",
    "        result = sum(cross_validate(clf, X, y=y, cv=cv, scoring=profit_scoring)['test_score'])\n",
    "        print(f\"{name}: test core = {result} \")\n",
    "def code_for_test(X,y):\n",
    "    \"\"\"\n",
    "    test code with 10-fold stratified cross validation\n",
    "    parameters\n",
    "    X: trainset features after generation\n",
    "    y: trainset y label\n",
    "    \"\"\"\n",
    "    evaluate_classification(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, columns):\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=300)\n",
    "    models = {'logistic': model, 'ridge': Ridge(alpha=1.0), 'Lasso': Lasso()} #'RFE': RFE(model, 5)\n",
    "    for k, v in models.items():\n",
    "        print(k)\n",
    "        fit = v.fit(X, y)\n",
    "#         if k == 'RFE':\n",
    "#             print(\"Num Features: %s\" % (fit.n_features_))\n",
    "#             print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "#             for i, j in sorted(zip(fit.ranking_, columns), reverse=False):\n",
    "#                 print(i,j)\n",
    "        if k == 'logistic':\n",
    "            for i, j in sorted(zip(map(lambda x: round(x, 4), fit.coef_.reshape(-1)), columns), reverse=True):\n",
    "                print(i,j)\n",
    "#         else:\n",
    "#             for i, j in sorted(zip(map(lambda x: round(x, 4), abs(fit.coef_)), columns), reverse=True):\n",
    "#                 print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for automatic feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_set = ['add', 'sub', 'mul', 'div', 'log', 'sqrt', 'abs', 'neg', 'max', 'min']  # \n",
    "def gp(X, y, gen, n_com):\n",
    "    gp1 = SymbolicTransformer(generations=gen, population_size=1000,\n",
    "                             hall_of_fame=1000, n_components=n_com,\n",
    "                             function_set=function_set,\n",
    "                             parsimony_coefficient=0.0005,\n",
    "                             max_samples=0.9, verbose=1,\n",
    "                             random_state=42, n_jobs=3,\n",
    "                             feature_names=X.columns)\n",
    "    x_gp = gp1.fit_transform(X, y)\n",
    "    code_for_test(x_gp, y)\n",
    "    return gp1, x_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X_train, X_test, y_train, clf):\n",
    "    pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def cv_cost_semi(X, X_test_other, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    cost = {\"RBF SVM\": 0,\n",
    "    \"Logistic Regression\": 0, \n",
    "    \"Neural Net\": 0,\n",
    "    \"Random Forest\": 0,\n",
    "    \"AdaBoost\" : 0,\n",
    "    \"XGBoost\": 0}\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # get the split\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # generate coresponding test label\n",
    "        # model = LabelSpreading(gamma=10)\n",
    "        # model = LabelPropagation(alpha=None, gamma=10, kernel='rbf', max_iter=1000,n_jobs=None, n_neighbors=7, tol=0.001)\n",
    "        model = LabelSpreading(alpha=0.2, gamma=10, kernel='rbf', max_iter=30, n_jobs=None,n_neighbors=7, tol=0.001)\n",
    "        y_new_label = model.fit(X_train, y_train).predict(X_test_other)\n",
    "        X_all = X_train.append(X_test_other)\n",
    "        \n",
    "        y_all = y_train.append(pd.DataFrame(y_new_label))\n",
    "        # evaluation \n",
    "        for name, clf in classifiers.items():\n",
    "            pred = classify(X_all, X_test, y_all, clf)\n",
    "            cost[name] += profit_scorer(y_test, pred>0.5)\n",
    "    print(f'cost = {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for cost-sensitive xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_obj(y_hat, dtrain, alpha=5, beta=25): # alpha for FN beta for FP\n",
    "    y = dtrain.get_label()\n",
    "    pred = 1. / (1. + np.exp(-y_hat))\n",
    "    grad = pred * (beta + alpha*y - beta*y) - alpha*y  # alpha*(p-1)*y + beta*p*(1-y)\n",
    "    hess = pred * (1 - pred) * (beta + alpha*y - beta*y)\n",
    "    return grad, hess\n",
    "\n",
    "def err_rate(pred, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    loss_fn = y*np.log(pred)\n",
    "    loss_fp = (1.0 - y)*np.log(1.0 - pred)\n",
    "    return 'error', np.sum(-(5*loss_fn+25*loss_fp))/len(y)\n",
    "\n",
    "def cross_validation(X_train, X_test, y_train, y_test, depth, num_round):\n",
    "    # load data\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_val = xgb.DMatrix(X_test, label=y_test)\n",
    "    # build model\n",
    "    param = {'max_depth': depth, 'eta': 0.2, 'silent': 1, 'seed': 42, 'scale_pos_weight':1}\n",
    "#     watchlist = [(d_val, 'eval'), (d_train, 'train')]\n",
    "#     model_trn = xgb.train(param, d_train, num_round, watchlist, obj=logistic_obj, feval=err_rate)\n",
    "    model_trn = xgb.train(param, d_train, num_round, obj=logistic_obj, feval=err_rate)\n",
    "    # prediction\n",
    "    pred = model_trn.predict(d_val) \n",
    "    pred = 1. / (1. + np.exp(-pred))\n",
    "    return pred\n",
    "def cv_cost_xg(X,y, depth, rounds):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "    cost=0\n",
    "    if type(X) == pd.DataFrame:\n",
    "        X=X.values\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pred = cross_validation(X_train, X_test, y_train, y_test, depth, rounds)\n",
    "#         print(f' confusion matrix: cost({profit_scorer(y_test, pred>0.5)})\\n {confusion_matrix(y_test, pred>0.5)}')\n",
    "        cost += profit_scorer(y_test, pred>0.5)\n",
    "\n",
    "    print(f'cost = {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for test\n",
    "prepare different input dataset and test at 10-fold stratified cross validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. X (raw data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 1879 entries and 10 features\n",
      "Test set has 498121 entries and 9 features\n"
     ]
    }
   ],
   "source": [
    "#raw data\n",
    "train_data = pd.read_csv('data/train.csv', sep = '|')\n",
    "test_data = pd.read_csv('data/test.csv', sep = '|')\n",
    "print(f'Train set has {train_data.shape[0]} entries and {train_data.shape[1]} features')\n",
    "print(f'Test set has {test_data.shape[0]} entries and {test_data.shape[1]} features')\n",
    "y = train_data['fraud']\n",
    "X = train_data.drop(columns=['fraud']).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete correlate features\n",
    "X_manual = X.assign(no_item = X.totalScanTimeInSeconds* X.scannedLineItemsPerSecond)\\\n",
    "                     .drop(columns=['valuePerSecond', 'lineItemVoidsPerPosition','scannedLineItemsPerSecond'])\n",
    "X_test = test_data.assign(no_item = test_data.totalScanTimeInSeconds* test_data.scannedLineItemsPerSecond)\\\n",
    "                    .drop(columns=['valuePerSecond', 'lineItemVoidsPerPosition','scannedLineItemsPerSecond'])\n",
    "\n",
    "fit_minmax = MinMaxScaler()\n",
    "# normalize with encode\n",
    "X_manual_encode = pd.get_dummies(X_manual, columns=['trustLevel'], prefix='trustLevel')\n",
    "X_test_encode = pd.get_dummies(X_test, columns=['trustLevel'], prefix='trustLevel')\n",
    "X_train_manual_enc = pd.DataFrame(fit_minmax.fit_transform(X_manual_encode), columns=X_manual_encode.columns, index=X_manual_encode.index)\n",
    "X_test_manual_enc = pd.DataFrame(fit_minmax.transform(X_test_encode), columns=X_manual_encode.columns, index=X_test_encode.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = 205 \n",
      "Neural Net: test core = 180 \n",
      "XGBoost: test core = 80 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(X_train_manual_enc,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto generate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    11.86         0.094761        9         0.707305         0.651601      5.87m\n",
      "   1     9.58         0.276511       14         0.722464         0.744651      2.36m\n",
      "   2     8.42         0.469678       14         0.783092         0.807669      3.11m\n",
      "   3    11.46          0.63334       17         0.808567         0.733662      3.51m\n",
      "   4    15.33          0.69389       31         0.813072         0.626655      3.13m\n",
      "   5    17.37         0.712183       17         0.818479         0.644542      2.81m\n",
      "   6    18.83         0.732394       21         0.823698         0.546554      2.79m\n",
      "   7    19.90         0.747221       34         0.823915         0.659249      2.87m\n",
      "   8    19.84         0.742844       30         0.827044          0.59718      3.12m\n",
      "   9    21.02         0.740488       31         0.824565         0.717744      2.85m\n",
      "  10    22.25         0.746554       19         0.838084         0.493703      3.10m\n",
      "  11    22.36         0.738917       34         0.827467         0.671664      2.92m\n",
      "  12    24.18         0.741468       32         0.836429         0.576184      3.16m\n",
      "  13    26.13         0.749584       46         0.833621         0.591198      2.98m\n",
      "  14    26.94         0.744273       30         0.832481         0.435813      3.15m\n",
      "  15    28.73         0.744509       33         0.846065         0.722157      3.12m\n",
      "  16    31.51         0.732658       57         0.850064         0.789959      3.27m\n",
      "  17    38.60         0.755411       71         0.861799         0.766309      3.21m\n",
      "  18    44.67         0.749204       72         0.868726         0.689672      3.48m\n",
      "  19    46.56         0.756051       47         0.865465         0.696961      3.35m\n",
      "  20    44.01         0.748449       60         0.867603         0.659423      3.54m\n",
      "  21    43.03         0.753184       90         0.875551         0.709686      3.59m\n",
      "  22    45.70         0.742028       58         0.883662         0.808539      3.30m\n",
      "  23    46.77          0.75739       54         0.892129          0.70354      3.32m\n",
      "  24    52.40         0.755799       61         0.894628         0.743022      3.14m\n",
      "  25    56.09         0.781226       82          0.89397         0.806372      3.42m\n",
      "  26    58.02         0.777707       64         0.895447         0.736113      3.58m\n",
      "  27    59.95         0.784839      113         0.897958         0.844272      3.52m\n",
      "  28    58.44         0.782826       87         0.903513         0.793527      3.77m\n",
      "  29    58.49         0.786378       61         0.900483         0.727905      3.85m\n",
      "  30    59.44         0.782333       58         0.898721          0.82965      3.96m\n",
      "  31    58.98         0.792305       86          0.90048         0.642479      3.78m\n",
      "  32    57.46         0.790521       58         0.900004         0.815643      3.67m\n",
      "  33    59.55         0.787406       64         0.902434         0.596415      3.46m\n",
      "  34    59.75         0.786541      117         0.907439         0.910043      3.60m\n",
      "  35    59.58         0.786774       82         0.911093         0.785309      3.56m\n",
      "  36    59.86         0.784567       76         0.909101         0.850181      3.47m\n",
      "  37    63.88         0.791494      113         0.916658         0.850529      3.45m\n",
      "  38    61.06         0.794154       80         0.919694         0.779325      3.50m\n",
      "  39    63.91         0.797037       94         0.918988         0.867832      3.25m\n",
      "  40    63.58         0.800251       92         0.922618         0.824741      3.33m\n",
      "  41    66.22         0.797452       93         0.926399         0.841443      3.29m\n",
      "  42    66.06         0.805593      112         0.927722         0.848753      3.18m\n",
      "  43    67.33         0.810735       84         0.931747         0.812813      3.01m\n",
      "  44    66.19         0.811073       70         0.925198         0.856129      3.13m\n",
      "  45    66.31         0.811157      122         0.936718         0.821544      3.15m\n",
      "  46    67.65         0.811055       70         0.927351         0.835218      3.29m\n",
      "  47    66.82         0.804427      115         0.928749         0.868988      3.24m\n",
      "  48    66.38         0.807848      119         0.930937         0.888733      3.52m\n",
      "  49    64.76         0.797558       89         0.929507         0.867553      3.87m\n",
      "  50    65.67         0.804981      118         0.934219         0.861935      2.99m\n",
      "  51    66.18         0.808393      113         0.933255         0.888399      3.14m\n",
      "  52    65.93         0.814224       72         0.928948         0.667106      3.16m\n",
      "  53    64.23           0.8079       94         0.935105         0.886364      3.10m\n",
      "  54    64.88         0.813003       94         0.934817         0.889159      3.64m\n",
      "  55    64.61         0.810727      125         0.935516         0.772798      2.79m\n",
      "  56    65.03         0.811185       98         0.934245         0.916868      2.73m\n",
      "  57    63.72         0.800992       91           0.9347         0.783956      2.58m\n",
      "  58    62.91         0.799129       79          0.92901         0.790454      2.47m\n",
      "  59    63.30         0.805414      124         0.927962         0.854788      2.56m\n",
      "  60    61.32         0.793513      125         0.932075         0.889023      3.62m\n",
      "  61    60.48         0.796542      105          0.92892         0.872942      2.35m\n",
      "  62    59.97          0.79488      108         0.930733         0.853904      2.41m\n",
      "  63    57.61         0.784157       75         0.931114         0.835326      2.28m\n",
      "  64    55.29         0.777094       64         0.926278         0.846753      2.20m\n",
      "  65    55.41         0.791718      105         0.933556         0.882692      2.61m\n",
      "  66    55.43         0.790854       70         0.931059         0.866295      1.91m\n",
      "  67    54.18         0.780015      100         0.931609         0.794665      1.95m\n",
      "  68    54.90         0.781967       56         0.928658          0.83417      1.93m\n",
      "  69    52.67         0.769551       76         0.929898         0.795719      1.71m\n",
      "  70    51.17         0.777019       49         0.927594         0.810172      1.54m\n",
      "  71    51.39         0.774404       73         0.927614          0.92489      1.67m\n",
      "  72    52.02         0.779888       51         0.929635         0.709492      2.65m\n",
      "  73    50.97         0.762514       56         0.933098         0.712223      2.49m\n",
      "  74    51.83         0.777264       96         0.930084         0.899021      2.04m\n",
      "  75    51.30         0.775547       58         0.926421         0.835953      1.55m\n",
      "  76    49.95         0.773139       49         0.927092         0.777819      1.37m\n",
      "  77    49.62         0.770494       47         0.928207         0.718479      1.38m\n",
      "  78    50.06         0.769071       50         0.926018         0.888128      1.31m\n",
      "  79    50.67         0.770827       49          0.92608          0.84951      1.16m\n",
      "  80    50.58         0.766848       49         0.925787         0.796901      1.16m\n",
      "  81    49.37         0.762808       49         0.924861         0.800407      1.07m\n",
      "  82    49.66         0.775629       51         0.926941         0.845837     59.01s\n",
      "  83    48.99         0.754215       93         0.926806         0.847406     59.95s\n",
      "  84    49.80         0.773212      106         0.937091         0.867728     59.84s\n",
      "  85    49.84         0.766667       49          0.92544         0.842729      1.04m\n",
      "  86    48.76         0.758996       52          0.92545         0.860824     51.34s\n",
      "  87    50.27          0.77009       90         0.925791         0.861164     42.68s\n",
      "  88    51.60         0.772809      101          0.92995         0.856011     37.77s\n",
      "  89    50.44         0.768987      104         0.931084          0.83502     33.32s\n",
      "  90    50.32         0.769384       56         0.925415         0.831891     55.36s\n",
      "  91    49.46         0.762435       49         0.927643          0.79841     32.00s\n",
      "  92    47.83         0.758726       91         0.932408         0.881593     21.75s\n",
      "  93    49.96         0.761948       87         0.931054         0.931789     21.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94    50.18         0.770984      106          0.92963          0.95864     17.25s\n",
      "  95    49.72         0.759817       70          0.93057          0.86936     12.62s\n",
      "  96    50.59         0.769957       50         0.933293         0.795149      9.74s\n",
      "  97    49.91          0.76969       70         0.934764         0.753906      6.78s\n",
      "  98    50.27          0.76229       74         0.937564         0.865825      3.16s\n",
      "  99    49.87         0.767216       70         0.938455         0.707139      0.00s\n",
      "Logistic Regression: test core = 310 \n",
      "Neural Net: test core = 350 \n",
      "XGBoost: test core = 275 \n"
     ]
    }
   ],
   "source": [
    "gpresult, xgp = gp(X_train_manual_enc, y, 100, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: test core = 205 \n",
      "Neural Net: test core = 180 \n",
      "XGBoost: test core = 80 \n"
     ]
    }
   ],
   "source": [
    "code_for_test(X_gp,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semi-supervised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_cost_semi(X_train_manual_enc, X_test_manual_enc.iloc[:,:30000], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define xgboost with cost sensitive\n",
    "1. better than original sgboost but worse than other algorithm \n",
    "2. best score 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_cost_xg(X_train_manual_enc, y, 3 ,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
